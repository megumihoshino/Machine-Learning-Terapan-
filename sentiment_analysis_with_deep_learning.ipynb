{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuo+Lsm67tvaMJfDXu1QwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megumihoshino/Machine-Learning-Terapan-/blob/main/sentiment_analysis_with_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwEXyPHs424B",
        "outputId": "889b2380-89aa-4ccd-da79-d31e24a6316e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjrNCGJ4rMAy",
        "outputId": "13940d71-470c-4ba5-a2b5-47501d90f46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/indobenchmark/indonlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znNVUGiYrWhN",
        "outputId": "3178f8a9-ecb5-476a-e8ce-710439176d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'indonlu' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from indonlu.utils.forward_fn import forward_sequence_classification\n",
        "from indonlu.utils.metrics import document_sentiment_metrics_fn\n",
        "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "42q_TbhVrk7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- set_seed: mengatur dan menetapkan random seed\n",
        "- count_param: ngitung jml parameter dlm model\n",
        "- get_lr: ngatur leanring rate\n",
        "- metrics_to_string: mengonversi metriks ke dlm string"
      ],
      "metadata": {
        "id": "dBnQqax0ZlAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#common functions\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "def count_param(module, trainable = False):\n",
        "  if trainable:\n",
        "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "  else:\n",
        "    return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    return param_group['lr']\n",
        "\n",
        "def metrics_to_string(metric_dict):\n",
        "  string_list = []\n",
        "  for key, value in metric_dict.items():\n",
        "    string_list.append('{}:{:.2f}'.format(key, value))\n",
        "  return ' '.join(string_list)"
      ],
      "metadata": {
        "id": "8mb16ola49Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(1112024)"
      ],
      "metadata": {
        "id": "cONSz7Z_5nMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KONFIGURASI DAN PRE-TRAINED MODEL**\n",
        "\n",
        "LOAD MODEL DAN KONFIGURASI\n",
        "- di thp ini, digunakan pre-trained moedl Indobert-base-p1 yg mempunyai 124,5 jt parameter.\n",
        "- model indobert dibangun berdsrkan general-purpose architecture BERT (bidirectiona; encoder representation from transformers). didesain utk memahami arti bhs ambigu dlm teks."
      ],
      "metadata": {
        "id": "LLYFT02CZ84I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load tokenizer n config\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
        "\n",
        "#instantiate model\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config = config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWmPi8ClaBnA",
        "outputId": "bc9330c9-2182-4979-be98-f401f2d82ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9skPfWzpbVBo",
        "outputId": "6cb78d28-b970-4f82-9f21-8349c07dea3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_param(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRiH-6X2bvIx",
        "outputId": "87d337de-e3c9-42db-d143-e1ab430e272f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124443651"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
        "valid_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
        "test_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
      ],
      "metadata": {
        "id": "UxPMaGUTcwmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTASI DATASET DAN DATALOADER DI PYTORCH"
      ],
      "metadata": {
        "id": "PEDFFscUepiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class DocumentSentimentDataset(Dataset):\n",
        "  #static constant variable\n",
        "  LABEL2INDEX = {'positive':0, 'neutral': 1, 'negative':2}\n",
        "  INDEX2LABEL = {0:'positive', 1:'neutral', 2:'negative'}\n",
        "  NUM_LABELS = 3 #jml label\n",
        "\n",
        "  def load_dataset(self, path):\n",
        "    df = pd.read_csv(path, sep = '\\t', header = None)\n",
        "    df.columns = ['text', 'sentiment'] #kasi nama kolom pd table\n",
        "    df['sentiment'] = df['sentiment'].apply(lambda lab: self.LABEL2INDEX[lab]) #ngonversi string label ke index\n",
        "    return df\n",
        "\n",
        "  def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
        "    self.data = self.load_dataset(dataset_path)\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    data = self.data.loc[index,:]\n",
        "    text, sentiment = data['text'], data['sentiment']\n",
        "    subwords = self.tokenizer.encode(text)\n",
        "\n",
        "    return np.array(subwords), np.array(sentiment), data['text']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BCg_O00SevWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTIMENT DATA LOADER"
      ],
      "metadata": {
        "id": "H6eqEe2N4AXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class DocumentSentimentDataLoader(DataLoader):\n",
        "  def __init__(self, max_seq_len = 512, *args, **kwargs):\n",
        "    super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
        "    self.max_seq_len = max_seq_len      #batas maksimum subword\n",
        "    self.collate_fn = self._collate_fn  #fingsi collate_fn dgn fungsi yg kita definisikan\n",
        "\n",
        "  def _collate_fn(self, batch):\n",
        "    batch_size = len(batch) #batch size\n",
        "    max_seq_len = max(map(lambda x: len(x[0]), batch)) #panjang subword maks dr batch\n",
        "    max_seq_len = min(self.max_seq_len, max_seq_len) #compare dgn batas yg ditentukan sblmnya\n",
        "\n",
        "#buat buffer utk subword, mask, dan sentimen labels, inisialisasi semua dgn 0\n",
        "    subword_batch = np.zeros((batch_size, max_seq_len), dtype= np.int64)\n",
        "    mask_batch = np.zeros((batch_size, max_seq_len), dtype = np.float32)\n",
        "    sentiment_batch = np.zeros((batch_size, 1), dtype = np.int64)\n",
        "\n",
        "#isi semua buffer\n",
        "    for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
        "      subwords = subwords[:max_seq_len]\n",
        "      subword_batch[i,:len(subwords)] = subwords\n",
        "      mask_batch[i,:len(subwords)] = 1\n",
        "      sentiment_batch[i,0] = sentiment\n",
        "\n",
        "#return to subword, mask n sentiment data\n",
        "    return subword_batch, mask_batch, sentiment_batch\n",
        "\n"
      ],
      "metadata": {
        "id": "wDmVPR6D4ESX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define variable for both classes\n",
        "\n",
        "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase= True)\n",
        "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase = True)\n",
        "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "train_loader = DocumentSentimentDataLoader(dataset = train_dataset, max_seq_len = 512, batch_size = 32, num_workers = 16, shuffle = True)\n",
        "valid_loader = DocumentSentimentDataLoader(dataset = valid_dataset, max_seq_len = 512, batch_size = 32, num_workers = 16, shuffle = False)\n",
        "test_loader = DocumentSentimentDataLoader(dataset = test_dataset, max_seq_len = 512, batch_size = 32, num_workers = 16, shuffle = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti20r06Ox3Hh",
        "outputId": "cb305297-ff67-45b5-8abd-8217ecefcea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxdNdksl2VKp",
        "outputId": "80c05781-36ba-4ebc-e4ee-070e2bca3227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([    2,  6540,    92,  2970,   213,  4259,  3553,   899,    34,\n",
            "         259,  5590,   262,  2558,   386,   899,  1687,    26,  1574,\n",
            "       30470,   899,  3310, 30468, 22130, 30360,  6123,  6368, 30468,\n",
            "       22130, 30360,  2652,  1746, 30468,  8869,  6540,    34,  6315,\n",
            "        1622,  1256,  8949,   899, 30468,  4222,  1622,   752,   245,\n",
            "         295,  2083, 30470,  2346,  7107,   300, 30470,   405,   724,\n",
            "        5189, 30470,   843, 17464,   899,   540, 10989,  3331,  1107,\n",
            "       30468,   119,  3221,    79,    34,  2170,    98,  9167, 30457,\n",
            "           3]), array(0), 'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define variabel (2)\n",
        "\n",
        "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ],
      "metadata": {
        "id": "IHW-UAio3qfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031494de-8373-4665-b220-d62954293b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
            "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MENGUJI MODEL DGN CTH KALIMAT"
      ],
      "metadata": {
        "id": "E41tws-VZW_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Senang mempelajari machine learning karena menambah wawasan baru'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1,-1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim = -1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text:{text} | Label: {i2w[label]} ({F.softmax(logits, dim =-1).squeeze()[label]* 100:.3f}%)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L80rZ4uVZWeB",
        "outputId": "8172f95e-ac1b-485f-e5c4-e959ed61a548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:Senang mempelajari machine learning karena menambah wawasan baru | Label: positive (36.283%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "modelnya msh salah memprediksi sentiment teks yang seharusnya memiliki sentiment positif, maka akan dilakukan proses fine tunning dan evaluasi."
      ],
      "metadata": {
        "id": "TTPQ60Wrav3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINE TUNING DAN EVALUASI\n"
      ],
      "metadata": {
        "id": "tOaNANuka-1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = 3e-6)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "5Fi3L6oybCLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN\n",
        "\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "  model.train()\n",
        "  torch.set_grad_enabled(True)\n",
        "\n",
        "  total_train_loss = 0\n",
        "  list_hyp, list_label = [], []\n",
        "\n",
        "  train_pbar = tqdm(train_loader, leave = True, total = len(train_loader))\n",
        "  for i, batch_data in enumerate(train_pbar):\n",
        "    #forward label\n",
        "    loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:3], i2w = i2w, device = 'cuda')\n",
        "\n",
        "    #update model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    tr_loss = loss.item()\n",
        "    total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "    #calculate metrics\n",
        "    list_hyp += batch_hyp\n",
        "    list_label += batch_label\n",
        "\n",
        "    train_pbar.set_description(\"(Epoch{}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),total_train_loss/(i+1), get_lr(optimizer)))\n",
        "\n",
        "  #calculate train metric\n",
        "  metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "  print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} lr: {:.8F}\".format((epoch+1), total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "\n",
        "  #evaluate on validation\n",
        "  model.eval()\n",
        "  torch.set_grad_enabled(False)\n",
        "\n",
        "  total_loss, total_correct, total_labels = 0, 0, 0\n",
        "  list_hyp, list_label = [], []\n",
        "\n",
        "  pbar = tqdm(valid_loader, leave = True, total = len(valid_loader))\n",
        "  for i, batch_data in enumerate(pbar):\n",
        "    batch_seq = batch_data[:3]\n",
        "    loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:3], i2w = i2w, device = 'cuda')\n",
        "\n",
        "    #calculate total loss\n",
        "    valid_loss = loss.item()\n",
        "    total_loss = total_loss + valid_loss\n",
        "\n",
        "    #calculate evaluation metrics\n",
        "    list_hyp += batch_hyp\n",
        "    list_label += batch_label\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "    pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
        "\n",
        "  metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "  print(\"(Epoch{}) VALID LOSS: {:.4f}{}\".format((epoch+1), total_loss/(i+1), metrics_to_string(metrics)))"
      ],
      "metadata": {
        "id": "6MgAhtxRdBkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792e24a4-422d-46fa-e26f-ebb59315ba53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch1) TRAIN LOSS:0.1616 LR:0.00000300: 100%|██████████| 344/344 [02:44<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.1616 ACC:0.94 F1:0.93 REC:0.92 PRE:0.93 lr: 0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "VALID LOSS:0.1743 ACC:0.93 F1:0.90 REC:0.90 PRE:0.92: 100%|██████████| 40/40 [00:07<00:00,  5.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch1) VALID LOSS: 0.1743ACC:0.93 F1:0.90 REC:0.90 PRE:0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/344 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "(Epoch2) TRAIN LOSS:0.1184 LR:0.00000300: 100%|██████████| 344/344 [02:42<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 2) TRAIN LOSS:0.1184 ACC:0.96 F1:0.95 REC:0.95 PRE:0.95 lr: 0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "VALID LOSS:0.1704 ACC:0.94 F1:0.91 REC:0.90 PRE:0.93: 100%|██████████| 40/40 [00:07<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch2) VALID LOSS: 0.1704ACC:0.94 F1:0.91 REC:0.90 PRE:0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/344 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "(Epoch3) TRAIN LOSS:0.0904 LR:0.00000300: 100%|██████████| 344/344 [02:42<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.0904 ACC:0.97 F1:0.96 REC:0.96 PRE:0.97 lr: 0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "VALID LOSS:0.1764 ACC:0.94 F1:0.92 REC:0.90 PRE:0.93: 100%|██████████| 40/40 [00:09<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch3) VALID LOSS: 0.1764ACC:0.94 F1:0.92 REC:0.90 PRE:0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/344 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "(Epoch4) TRAIN LOSS:0.0679 LR:0.00000300: 100%|██████████| 344/344 [02:42<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 4) TRAIN LOSS:0.0679 ACC:0.98 F1:0.97 REC:0.97 PRE:0.97 lr: 0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "VALID LOSS:0.1807 ACC:0.94 F1:0.92 REC:0.91 PRE:0.92: 100%|██████████| 40/40 [00:07<00:00,  5.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch4) VALID LOSS: 0.1807ACC:0.94 F1:0.92 REC:0.91 PRE:0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/344 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "(Epoch5) TRAIN LOSS:0.0485 LR:0.00000300: 100%|██████████| 344/344 [02:42<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.0485 ACC:0.99 F1:0.98 REC:0.98 PRE:0.99 lr: 0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "VALID LOSS:0.2098 ACC:0.93 F1:0.91 REC:0.91 PRE:0.92: 100%|██████████| 40/40 [00:08<00:00,  4.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch5) VALID LOSS: 0.2098ACC:0.93 F1:0.91 REC:0.91 PRE:0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUASI"
      ],
      "metadata": {
        "id": "04koGqfsK6Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EVALUATE ON TEST\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave = True, total = len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "  _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:3], i2w = i2w, device = 'cuda')\n",
        "  list_hyp += batch_hyp\n",
        "\n",
        "#saving prediction\n",
        "\n",
        "df = pd.DataFrame({'label': list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index = False)\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2w0HD4bK7s8",
        "outputId": "3e3ec238-c909-4e3c-c4fb-3b96e72954f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:02<00:00,  5.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     index     label\n",
            "0        0  negative\n",
            "1        1  negative\n",
            "2        2  negative\n",
            "3        3  negative\n",
            "4        4  negative\n",
            "..     ...       ...\n",
            "495    495   neutral\n",
            "496    496   neutral\n",
            "497    497   neutral\n",
            "498    498  positive\n",
            "499    499  positive\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PREDIKSI SENTIMENT\n",
        "\n",
        "text = 'Merasa kagum dengan toko ini tapi berubah menjadi kecewa setelah transaksi'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1,-1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim = -1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text:{text} | Label: {i2w[label]} ({F.softmax(logits, dim =-1).squeeze()[label]* 100:.3f}%)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udqPwx1gOING",
        "outputId": "af49ee55-aff8-4917-beb6-cee359dec0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:Merasa kagum dengan toko ini tapi berubah menjadi kecewa setelah transaksi | Label: negative (99.836%)\n"
          ]
        }
      ]
    }
  ]
}